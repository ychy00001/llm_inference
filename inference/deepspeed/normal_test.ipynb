{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80e8657-5d3f-4d13-ab49-87f123917e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0965534c-ac0c-457d-8e01-cf9b513008b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
    "world_size = int(os.getenv('WORLD_SIZE', '8'))\n",
    "world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b823cb-7759-4ef4-bfe8-ab1fcad5035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/data/yckj3980/tmp_model/bloomz-7b1-mt\"\n",
    "# max_memory_mapping = {6: \"1GB\", 7: \"2GB\"}\n",
    "# model_8bit = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", load_in_8bit=True, max_memory=max_memory_mapping)\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# generation = pipeline(\"text-generation\", model=model_8bit, tokenizer=tokenizer)\n",
    "\n",
    "#prompt = \"你是一个对话机器人，请用对话的方式回答下面问题。问题：你是谁？\"\n",
    "#inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#generated_ids = model_8bit.generate(**inputs)\n",
    "#outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "#outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8a54d4-78f4-49df-bd80-fdd1faebfa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-13 11:16:19,628] [INFO] [logging.py:93:log_dist] [Rank -1] DeepSpeed info: version=0.8.3, git-hash=unknown, git-branch=unknown\n",
      "[2023-04-13 11:16:19,633] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2023-04-13 11:16:19,634] [INFO] [logging.py:93:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2023-04-13 11:16:22,041] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2023-04-13 11:16:22,234] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.128.61.9, master_port=29500\n",
      "[2023-04-13 11:16:22,237] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the new group's world size should be less or equal to the world size set by init_process_group",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_engine \u001b[38;5;241m=\u001b[39m \u001b[43mdeepspeed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmp_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mreplace_with_kernel_inject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/deepspeed/__init__.py:311\u001b[0m, in \u001b[0;36minit_inference\u001b[0;34m(model, config, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m config_dict\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    309\u001b[0m ds_inference_config \u001b[38;5;241m=\u001b[39m DeepSpeedInferenceConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m--> 311\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mInferenceEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_inference_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/deepspeed/inference/engine.py:107\u001b[0m, in \u001b[0;36mInferenceEngine.__init__\u001b[0;34m(self, model, config)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmpu\u001b[38;5;241m.\u001b[39mget_model_parallel_group()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtensor_parallel\u001b[38;5;241m.\u001b[39mtp_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_model_parallel_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     config\u001b[38;5;241m.\u001b[39mtensor_parallel\u001b[38;5;241m.\u001b[39mtp_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_group\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/deepspeed/inference/engine.py:209\u001b[0m, in \u001b[0;36mInferenceEngine._create_model_parallel_group\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    206\u001b[0m     get_accelerator()\u001b[38;5;241m.\u001b[39mset_device(local_rank)\n\u001b[1;32m    208\u001b[0m     ranks \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mtensor_parallel\u001b[38;5;241m.\u001b[39mtp_size)]\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_group \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mranks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     InferenceEngine\u001b[38;5;241m.\u001b[39minference_mp_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_group\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/deepspeed/comm/comm.py:182\u001b[0m, in \u001b[0;36mnew_group\u001b[0;34m(ranks)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m cdb\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m cdb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cdb\u001b[38;5;241m.\u001b[39mis_initialized(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepSpeed backend not set, please initialize it using init_process_group()\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mranks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/deepspeed/comm/torch.py:195\u001b[0m, in \u001b[0;36mTorchBackend.new_group\u001b[0;34m(self, ranks)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_group\u001b[39m(\u001b[38;5;28mself\u001b[39m, ranks):\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mranks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3481\u001b[0m, in \u001b[0;36mnew_group\u001b[0;34m(ranks, timeout, backend, pg_options)\u001b[0m\n\u001b[1;32m   3479\u001b[0m group_world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ranks)\n\u001b[1;32m   3480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group_world_size \u001b[38;5;241m>\u001b[39m global_world_size:\n\u001b[0;32m-> 3481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   3482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe new group\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms world size should be less or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the world size set by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_process_group\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3485\u001b[0m     )\n\u001b[1;32m   3486\u001b[0m \u001b[38;5;66;03m# check ranks' sanity\u001b[39;00m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank \u001b[38;5;129;01min\u001b[39;00m ranks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the new group's world size should be less or equal to the world size set by init_process_group"
     ]
    }
   ],
   "source": [
    "ds_engine = deepspeed.init_inference(model_8bit,\n",
    "                                 mp_size=4,\n",
    "                                 dtype=torch.half,\n",
    "                                 checkpoint=None,\n",
    "                                 replace_with_kernel_inject=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201f678-8522-4eb9-983e-7e115ce782df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ds_engine.module\n",
    "output = model('你好')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f970c9e-bcec-4000-be90-0a317eb2a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_inference",
   "language": "python",
   "name": "llm_inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
