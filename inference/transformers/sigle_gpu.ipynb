{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d1498e-2ff7-43e0-8760-e071b49ba8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62c9891-d0bd-4a10-9a99-5207e5ed5a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda114.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/yckj3980/miniconda3 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64/mpich/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('mpi/mpich-x86_64')}\n",
      "  warn(msg)\n",
      "/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('%S\"` $user\\\\($realUser\\\\)@$ip[PID'), PosixPath('{ code=$?;thisHistID=`history 1|awk \"{print \\\\\\\\$1}\"`;lastCommand=`history 1| awk \"{\\\\\\\\$1=\\\\\"\\\\\" ;print}\"`;user=`id -un`;whoStr=(`who -u am i`);realUser=${whoStr[0]};logDay=${whoStr[2]};logTime=${whoStr[3]};pid=${whoStr[5]};ip=${whoStr[6]};if [ ${thisHistID}x != ${lastHistID}x ];then echo -E `date \"+%Y/%m/%d %H'), PosixPath('$pid][LOGIN'), PosixPath('%M'), PosixPath('$logDay $logTime] --- [$PWD]$lastCommand [$code];lastHistID=$thisHistID;fi; } >> $HISTORY_FILE')}\n",
      "  warn(msg)\n",
      "/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('%Y/%m/%d %T   ')}\n",
      "  warn(msg)\n",
      "/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  eval `/usr/bin/modulecmd bash $*`\\n}')}\n",
      "  warn(msg)\n",
      "/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('inference/transformers/776b64ba-e6a3-46fc-b067-de1163765bc4')}\n",
      "  warn(msg)\n",
      "/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/data/yckj3980/tmp_model/bloomz-7b1-mt\"\n",
    "# max_memory_mapping = {6: \"1GB\", 7: \"2GB\"}\n",
    "# model_8bit = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", load_in_8bit=True, max_memory=max_memory_mapping)\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954e57fa-d20e-41d0-b300-3e53a527d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yckj3980/llm_inference/venv/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['你是一个对话机器人，请用对话的方式回答下面问题。问题：你是谁？']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"你是一个对话机器人，请用对话的方式回答下面问题。问题：你是谁？\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model_8bit.generate(**inputs)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b80e8-db15-4223-8466-e125eb00b5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6d078-c559-4490-ac93-db4ee2c90837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_inference",
   "language": "python",
   "name": "llm_inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
